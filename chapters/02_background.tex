\chapter{Background}
\label{ch:background}

\section{Fixed-point iteration}
% TODO drop this?

Considering a function \(\varphi\colon M \to M\) which maps \(M\) to itself and a start point \(x_0 \in M\). The sequence \((x_k)_{k \in \mathbb{N}_0}\) generated by the fixed-point iteration is defined by \(x_{k+1} = \varphi(x_k)\) with \(k \in \mathbb{N}_0\).

The Banach fixed-point theorem guarantees that the sequence converges to the single fixed-point \(\varphi(x) = x\) if \(M\) is a complete metric space and \(\varphi\) is a contraction.

\section{DMFT}
% TODO dmft, self-consistency, iterative solver, -> complex valued fixpoint problem
% TODO monte carlo?
% TODO second quantisation?

\section{Mixing}
% TODO we want to guess a more effective trial -> a trial that is closer to the true value -> extrapolation
% TODO linear mixing, under/over relaxation, utilizes only last trial and result
% TODO generalized mixer could use multiple trials and results

\section{DIIS}
% TODO motivation + derivation, formulas, least squares interpretation (svd?)
% TODO define trial and result

The direct inversion of the iterative subspace (DIIS), also known as Pulay mixing, is an extrapolation technique developed by Peter Pulay. His intention was to accelerate and stabelize the convergence of the Hartree-Fock self-consistent field method.

DIIS utilizes trials and results of multiple previous fixed-point iterations to construct a linear combination of them to extrapolate a new trial for the next iteration. The coefficients are determined by a least squares optimization.

We consider a fixed-point problem \(g(x) = x\) with \(g\colon \mathbb{R}^N \to \mathbb{R}^N\). The form \[x_{k+1} = x_k + \beta f_k\] with the relaxation parameter \(\beta \in \mathbb{R}\) and the \(k\)th residual \(f_k = g(x_k) - x_k\), is a common approach to increase stability by under-relaxation with \(0 < \beta < 1\), or rate of convergence by over-relaxation with \(1 < \beta\). This technique is only using one trial and one result and can converge slowly. Pulay's DIIS attempts to guess a better trial by using multiple previous trials and results. \[x_{k+1} = \overline{x}_k + \beta \overline{f}_k\] \[\overline{x}_{k} = x_k - \sum_{j=1}^{m} \gamma_j {\Delta x}_{k-m+j}\] \[\overline{f}_{k} = f_k - \sum_{j=1}^{m} \gamma_j {\Delta f}_{k-m+j}\] with \({\Delta x}_{k} = x_k - x_{k-1}\), \({\Delta f}_{k} = f_k - f_{k-1}\). Where \(\overline{x}_{k}\) and \(\overline{f}_{k}\) represent a weighted sum over the previous \((m+1)\) trials and results.

Variations of the DIIS method exist to increase the rate of convergence and stability. The restarted Pulay method\cite{diis_restarted} and the periodic Pulay method\cite{diis_periodic} reset the state or use linear mixing inbetween to reduce side effects like divergence.

