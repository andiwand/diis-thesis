\chapter{Theoretical Background}
\label{ch:background}

\section{DMFT}
% TODO explain what x is in our case

Dynamical mean-field theory (DMFT) is a method to determine the electronic structure of strongly correlated materials. It can be solved by a self-consistency iteration shown in figure \ref{fig:dmft}.
It is basically a fixed-point iteration \(\varphi(x^\ast) = x^\ast\) with a very complicated, non-linear function \(\varphi\). The questions about existence and unambiguity have no general answer yet.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/dmft.png}
    \caption{Self-consistency loop of dynamical mean field theory for a Hubbard-type model. Image taken from Markus Wallerberger's dissertation.}
    \label{fig:dmft}
\end{figure}

\section{Fixed-point iteration}
% TODO drop this?
% TODO define trial and result

Consider a function \(\varphi\colon M \to M\) and a starting point \(x_0 \in M\). The sequence \((x_k)_{k \in \mathbb{N}_0}\) generated by the fixed-point iteration is defined by \(x_{k+1} = \varphi(x_k)\) with \(k \in \mathbb{N}_0\).

The Banach fixed-point theorem guarantees that the sequence converges to the single fixed-point \(\varphi(x^\ast) = x^\ast\) if \(M\) is a complete metric space and \(\varphi\) is a contraction.

\section{Mixing}
Mixing is a technique to improve convergence for self-consistency iteration. The idea is to construct a more effective trial with more information than just the previous result. To be effective such a trial has to be closer to the true value which means we want to extrapolate in terms of iterations.

The simplest approach is linear mixing
\begin{equation} \label{eq:linmix}
\hat{x}_k = (1-\beta) x_k + \beta \varphi(x_k) = x_k + \beta (\varphi(x_k) - x_k)
\end{equation}
with the relaxation parameter \(\beta\).
For \(0 < \beta < 1\) we get an under-relaxation which slows down convergence in some cases, but also dampens oscillations. If the self-consistency iterations diverge, the use of under-relaxation might help out.
For \(\beta > 1\) we get over-relaxation which can have predictive behaviour and speed up the convergence but it also amplifies errors and might result in divergence.

More complex mixing techniques use trials and results of multiple previous iterations and try to combine under-relaxation for robustness and prediction to increase the rate of convergence.

\section{DIIS}
The direct inversion of the iterative subspace (DIIS), also known as Pulay mixing, is an extrapolation technique developed by Peter Pulay. His intention was to accelerate and stabelize the convergence of the Hartree-Fock self-consistent field method.\cite{diis_pulay1}\cite{diis_pulay2} Besides that, DIIS was successfully applied to accelerate the Self-Consistent Field (SCF) approach for Density Functional Theory (DFT) calculations.\cite{diis_restarted}

DIIS uses trials and results of multiple previous iterations, constructs a linear combination of them, and extrapolates a new trial for the next iteration. The coefficients are determined by a least squares optimization.

Pulay's DIIS attempts to guess a better trial by using multiple previous trials and results. The method assumes that a good approximation of the true value \(x^\ast\) can be obtained by a linear combination of the previous trials.
\begin{equation} \label{eq:diis_x}
\overline{x}_{k} = \sum_{j=0}^{m} c_j x_{k-j}
\end{equation}
where \(m\) is the number of previous trials to consider. We can also write this sum in terms of the true value \(x^\ast\) and an error vector \(e_{k} = x^\ast - x_{k}\).
\[\overline{x}_{k} = \sum_{j=0}^{m} c_j (x^\ast + e_{k-j}) = x^\ast \sum_{j=0}^{m} c_j + \sum_{j=0}^{m} c_j e_{k-j}\]
To get close to the real value \(x^\ast\) we set \(\sum_{j=0}^{m} c_j = 1\) and try to minimize the second term \(\sum_{j=0}^{m} c_j e_{k-j}\). But since we do not know the true value \(x^\ast\) we cannot know \(e_{k}\). Therefore, we approximate \(e_{k}\) by the residuals, i.e. \(e_{k} \approx f_{k} = g(x_k) - x_k\).
\[\overline{f}_{k} = \sum_{j=0}^{m} c_j f_{k-j}\]
In order to minimize \(\overline{f}_{k}\) we use the \(l^2\)-norm.

One solution would be to use the Lagrange multiplier technique to satisfy the constraint \(\sum_{j=0}^{m} c_j = 1\) and minimize \(|\overline{f}_{k}|^2\).

However, we can embed the constraint by rewriting \footnotemark
\begin{equation} \label{eq:diis_f}
\overline{f}_{k} = f_k - \sum_{j=1}^{m} \gamma_j {\Delta f}_{k-m+j} = f_k - F_k \Gamma_k
\end{equation}
with \({\Delta f}_{k} = f_k - f_{k-1}\), \(F_k = [{\Delta f}_{k-m+j}]_{j=1..m}\) and \(\Gamma_k = [\gamma_j]_{j=1..m}\).

\footnotetext{For example with \(m=2\) we get \(\overline{f}_{k} = f_k - \gamma_1 {\Delta f}_{k-1} - \gamma_2 {\Delta f}_{k}\ = f_k - \gamma_1 (f_{k-1} - f_{k-2}) - \gamma_2 (f_k - f_{k-1}) = (1-\gamma_2) f_k + (\gamma_2 - \gamma_1) f_{k-1} + \gamma_1 f_{k-2} = c_0 f_k + c_1 f_{k-1} + c_2 f_{k-2}\). Equating coefficients leads to \(\sum_{j=0}^{m} c_j = 1\).}

We minimize \(|\overline{f}_{k}|^2\) by \(\Gamma_k\) and get the solution \footnotemark
\begin{equation} \label{eq:diis_gamma}
\Gamma_k = (F_k^\dagger F_k)^{-1} F_k^\dagger f_k = F_k^+ f_k
\end{equation}

\footnotetext{The last equals is only true if \(F_k^\dagger F_k\) is invertible. The Mooreâ€“Penrose inverse exists even if that is not the case.}

Similar to \eqref{eq:linmix} we can contruct a new trial with \eqref{eq:diis_x}, \eqref{eq:diis_f} and \eqref{eq:diis_gamma}.
\begin{equation} \label{eq:diis_final}
x_{k+1} = \overline{x}_k + \beta \overline{f}_k = x_k + \beta f_k - (X_k + \beta F_k) (F_k^\dagger F_k)^{-1} F_k^\dagger f_k
\end{equation}

Variations of the DIIS method exist to further increase the rate of convergence and stability. The restarted Pulay method\cite{diis_restarted} and the periodic Pulay method\cite{diis_periodic} reset the state or use linear mixing inbetween to reduce side effects of DIIS.

% TODO least squares interpretation

